# Attention Is All You Need — From Scratch (PyTorch)

This repository showcases a **from-scratch implementation of the original Transformer
architecture** introduced in the paper *Attention Is All You Need* (Vaswani et al., 2017).

The goal of this project is to demonstrate **deep architectural understanding**.
All components — attention, masking, encoder, decoder, and training —
are implemented manually in PyTorch **without using high-level Transformer libraries**.

This repository is intended as a **clean, educational reference**
for understanding how Transformers work internally.

